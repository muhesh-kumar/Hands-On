{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 22\n",
    "___\n",
    "## Python Web Scraping\n",
    "___\n",
    "### What is Web Scrapping\n",
    "The interent is full of huge amount of data which can be used for different purposes. To collect this data we need to know to scrape data from a website.\n",
    "Web scraping is the process of extracting and collecting data from websites and storing it on a local machine or in a database.\n",
    "In this section, we will use beautifulsoup and requests package to scrape data. The package version we are using is beautifulsoup 4.\n",
    "To start scraping websites you need *requests, beautifulSoup4* and *website.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests\n",
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To scrape data from websites, basic understanding of HTML tags and css seelectors is needed. We target content from a website using HTML tags, classes or/and inds. Let's import the requests and BeautifulSoup module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's declare url variable for the website which are going to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = 'http://mlr.cs.umass.edu/ml/datasets.html'\n",
    "\n",
    "# Lets use the requests get method to fetch the data from url\n",
    "\n",
    "response = requests.get(url)\n",
    "# lets check the status\n",
    "status = response.status_code\n",
    "print(status) # 200 means the fetching was successful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using beautifulSoup to parse content from the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = 'http://mlr.cs.umass.edu/ml/datasets.html'\n",
    "\n",
    "response = requests.get(url)\n",
    "content = response.content # we get all the content from the website\n",
    "soup = BeautifulSoup(content, 'html.parser') # beautiful soup will give a chance to parse\n",
    "print(soup.title) # <title>UCI Machine Learning Repository: Data Sets</title>\n",
    "print(soup.title.get_text()) # UCI Machine Learning Repository: Data Sets\n",
    "print(soup.body) # gives the whole page on the website\n",
    "print(response.status_code)\n",
    "\n",
    "tables = soup.find_all('table', {'cellpadding':'3'})\n",
    "# We are targeting the table with cellpadding attribute with the value of 3\n",
    "# We can select using id, class or HTML tag , for more information check the beautifulsoup doc\n",
    "table = tables[0] # the result is a list, we are taking out data from it\n",
    "for td in table.find('tr').find_all('td'):\n",
    "    print(td.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "If you run this code, you can see that the extraction is half done. You can continue doing it because it is part of exercise 1. For reference check the beautifulsoup documentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
